## 分类——检查 ##

基于之前章节的设想，我们致力于建设一个基于“提问——回答”模式的网站。在有了基于聚类的方法之后，我们可以收集用户的答案之后进行分析。 我们接下来的一个目标是实现即时性地答案评定——在用户作答的时候，我们就能够即时性地进行评价。

## 学习分类 ##

在分类的时候，我们往往想要找出数据的标签。 为了达成这项工作，我们要回答一下两个问题：

- 我们应该如何表达数据？
- 我们应该选择什么样的模型或者结构？

## 矫正数据 ##

我们的数据，简单地讲，是由回答构成的文档数据；和一个由二元变量构成的标签数据——表示提问者是否接受文档作为问题的答案。 而文本类型的数据是一种十分不方便的数据表达形式。 所以，要使用特征提取方法将文本数据转化为适当的数值型数据，从而合适地运用机器学习算法。

## 矫正分类器 ##

对于潜在的分类器模型，我们实际上有很多的选择余地：支持向量机、 朴素贝叶斯分类器。 在本章节中，我们将要比较logistic回归和前面章节的基于实例的算法之间的优劣。

## 提取数据 ##

暂略

## 预选特征 ##

我们还应该只留下那些能够帮助分类器来区别好答案和坏答案的特征。 我们还需要身份特征来将问题和答案对应起来。 以下是我们的选择方案：

- PostType： 只是用来区分条目是问题还是答案，可以保留，全部设置为1
- CreationDate: 该特征能够表达从问题被提交到答案被提交之间的时间跨度，保留
- Score： 能够表达整个社区对答案的评价，保留
- ViewCount： 此特征不是在最初答案被提交的时候就生成的，剔除
- Body： 保留，但是需要从HTML中提取为纯文本
- OwnerUserId: 剔除，这里并不打算分析基于客户的特征
- Title： 剔除
- CommentCount: 剔除，剔除理由和ViewCount相同，并非提交后即产生的数据
- AcceptedAnswerId: 转化为二元变量，IsAccepted

## 确定优秀答案的标准 ##

利用来自整个社群的数据Score来作为答案优秀与否的标准。 正得分的答案我们标记为positive; 0得分和负得分的答案我们标记为negative。


## 构建分类器：kNN分类器 ##

我们先着手建立一个简单的分类器，kNN分类器。 之后，我们会运用Logistic方法进行重新分类，这样我们就能比较出二者分类效果上的差异。 我们先利用sklearn建立一个2NN分类器：

	>>> from sklearn import neighbors
	>>> knn = neighbors.KNeighborsClassifier(n_neighbors=2)
	>>> print(knn)
	KNeighborsClassifier(algorithm=auto, leaf_size=30, n_neighbors=2, p=2,
	warn_on_equidistant=True, weights=uniform)

sklearn模块下的所有模型都一样，利用fit方法进行训练，利用transform方法或者predict方法进行预测。如这里我们就可以用predict方法进行预测:

    >>> knn.fit([[1],[2],[3],[4],[5],[6]], [0,0,0,1,1,1])
	>>> knn.predict(1.5)
	array([0])
	>>> knn.predict(37)
	array([1])

predict_proba()方法则允许我们查看某个观测归属于某个类的时候计算得到的概率：

	>>> knn.predict_proba(1.5)
	array([[ 1., 0.]])
	>>> knn.predict_proba(37)
	array([[ 0., 1.]])
	>>> knn.predict_proba(3.5)
	array([[ 0.5, 0.5]])

## 特征工程（Y的确定） ##

什么样的特征才能被分类器用于分类？什么样的特征能达到比较高的区分度？ TimeToAnswer特征已经出现在了我们的meta字典中，但是对于分类工作未必有太大的作用。 Text特征是字符型的，我们的分类器只能输入数值型变量，所以要进行更改。 同时还可以对LinkCount变量制作条形图，可以发现大多数的观测根本就没有LinkCount数，也就是没有引用链接，所以这个特征也不能被分类器所利用。

## 训练分类器 ##

	X = np.asarray([extract_features_from_body(text) for post_id,
	text in fetch_posts() if post_id in all_answers])
	knn = neighbors.KNeighborsClassifier()
	knn.fit(X, Y)

这样，我们就可以将之前选定的X矩阵和Y特征一同输入kNN分类器进行训练。 这里我们直接进行5NN的训练，至于这个参数的确定，我们将在后面讲述。

## 度量分类器的效果 ##

首先应该明确究竟要度量什么？最简单的方法就是直接度量分类器在测试集上平均预测效果。这是一个0,1之间的数字。我们可以利用knn.score()来准确计算。 但是我们在此处并不会进行单一标准的评判，我们将会使用交叉验证技术。 利用sklearn.cross_validation中现成的KFold类来实现。最后，我们会计算整个测试集上每个fold的平均得分，并且会使用标准差来度量结果的变异性质。

	from sklearn.cross_validation import KFold
	scores = []

载入模块并设置空列表

	cv = KFold(n=len(X), k=10, indices=True)

配置交叉验证的取样方案

	for train, test in cv:
		X_train, y_train = X[train], Y[train]
		X_test, y_test = X[test], Y[test]

迭代地对每种取样方案进行取样

	clf = neighbors.KNeighborsClassifier()
	clf.fit(X, Y)
	scores.append(clf.score(X_test, y_test))
	print("Mean(scores)=%.5f\tStddev(scores)=%.5f"%(np.mean(scores, np.std(scores)))

训练分类器并且计算每种方案下的得分。

	Mean(scores)=0.49100 Stddev(scores)=0.0288

这里给出的结果就是统计量scores的均值和标准差。 这个分类效果十分地差，甚至低于百分之五十。 很明显，每个post中的链接数变量并不能十分有效地反映该post的类别信息，不能称得上有很高的区分度。

## 设计更多的特征 ##

除了考虑答案中的链接数目，我们还可以考虑答案中的代码数量和字数，者而这的量越大，也可以说明作答者更佳认真。加入这两个特征之后，我们的预测更佳精确了：

    Mean(scores)=0.58300 Stddev(scores)=0.02216

虽然绝对评分还是不高，但是更多的特征带来更精确的预测的方向是正确的。 所以我们下面将要加入更多的特征。 我们主要考虑如下四个新特征：

- 平均句子长度
- 平均用词长度
- 全大写的单词数量
- 感叹号的数量

![](http://i.imgur.com/YmHM3kr.png)

我们加入这四个特征之后，我们再一次地计算评分：

    Mean(scores)=0.57650 Stddev(scores)=0.03557

结果平均值减小了，同时标准差更大了。 为什么？

为了理解这个现象，我们还要回过去从kNN算法的原理开始讨论。 kNN先计算和test中向量最近的5个train向量（欧几里得距离），之后根据这5个向量中类别的分布来确定test向量的类别信息。 我们仔细观察可以发现，分类器自动地设置了p=2的默认参数。 这对应了闵科夫距离。 这意味着，分类器认为向量的7个特征是相似的。 但是各个特征对于最后分类器判定过程的贡献当然是不同的。 比如说，每个特征的取值范围就不同，这个差异性会影响到对于临近向量值的判断。

## 决定改进方案 ##

- 引入更多的数据
- 让模型更佳复杂：比如减小k值，使得模型对于不平滑的数据拟合效果更好；或者通过增加k值来达到反向的目的
- 修正特征空间：引入或减少特征，或者修改特征的取值范围
- 改变模型：也许kNN模型并不适合这个数据集

一般情况下，人们会依照上述的四大方向逐个地进行调试。 但是在这里，我们先不进行调试，先了解一个新的概念。

## Bias-variance 和 trade-off ##

在第一章中，我们试图通过拟合不同次数的多项式来实现不同复杂度的模型。 我们当时发现，直线模型并不好，因为数据本身并不是线性的。 我们称这种情况下，模型是欠拟合的，too biased。	

相反，当我们用高阶多项式进行拟合的时候，我们拟合地非常好。 但是对于不同的数据集，多项式的系数（模型参数）完全不同。 这说明我们的模型进入了过拟合状态，too high a variance for the given data。

事实上，bias 和 variance是模型精确度的两个极端，只能权衡而不能兼得。

## 降低高bias ##

这时候，无论升高还是降低数据的维度都无济于事。 唯一的原因就是模型过于简单。 此时应该考虑更换或者改进模型。

## 降低高variance ##

此时应选择降低模型的复杂度：加入更多地观测、 减少维度。 比如在kNN算法中就可以提高k值。

## 高Bias？还是低Bias ##

为了发现模型的问题所在，我们常常需要画出训练集和测试集上的模型误差随数据规模的变化曲线。 这里可以看到所谓高bias的特征，就是:测试误差在最开始先下降后上升，同时训练误差随着数据规模的升高而升高。 高variance的特征就是，这两条曲线之间的距离非常大。

![](http://i.imgur.com/BWqzqo1.png)

从此图可以看出，数据量的增大并不能减少模型的variance，所以并不能通过增加数据来改进模型。 接下来讨论是否能够修改特征空间。 我们在此只保留了LinkCount和NumTextTokens两个特征，之后按照原来的方案进行绘图。 发现变化并不显著：

![](http://i.imgur.com/4rVHGE8.png)

对于k值的调试会给出一些正面的效果，但是作用实在不大，而且我们为此付出了大量的计算时间上的代价。 平均得分最终稳定在0.62附近。

![](http://i.imgur.com/6PD03CN.png)

除此之外，kNN还是一个基于实例的方法。 这就意味着，面对日益增加的post数量，我们必须全数保留来维持模型的准确度，然而那样所需要的内存是无限的。

## 使用Logistic回归 ##

与名字不同，Logistic回归是一种分类方法，尤其是对于基于文本的分类任务，效果很好。 这得益于使用了logistic函数进行回归。

	>>> from sklearn.linear_model import LogisticRegression
	>>> clf = LogisticRegression()

导入logistic回归：

	>>> print(clf)
	LogisticRegression(C=1.0, class_weight=None, dual=False, fit_
	intercept=True, intercept_scaling=1, penalty=l2, tol=0.0001)

初始化logistic回归的信息：

	>>> clf.fit(X, y)
	>>> print(np.exp(clf.intercept_), np.exp(clf.coef_.ravel()))
	[ 0.09437188] [ 1.80094112]

模型的训练：

	>>> def lr_model(clf, X):
	return 1 / (1 + np.exp(-(clf.intercept_ + clf.coef_*X)))

生成logistic函数。

接下来我们观察logistic回归对于我们数据集的分类效果：

![](http://i.imgur.com/QVdegkj.png)

可见，此时模型的bias仍然十分高。 训练误差和测试误差汇聚到0.4左右。 说明我们的模型存在欠拟合的问题，数据的特征没有被完全捕捉。

这说明我们的数据——特征选择存在比较大的问题。

## 精度背后——precision和recall ##

让我们回过头来思考我们的目标，我们会发现我们并不需要一个十分精确的分类器，同时基于错误率，我们现在也得不到一个优秀的分类器。 如果我们的模型能够准确地预测某一类答案，那么就是成功的（能够准确预测出什么是优秀答案即可）。相反，一个总能够准确预测出坏答案的分类器对用户的帮助并不大。 为了查看我们的分类器现在的状态，我们应该学会如何求算precision和recall。 我们首先观察下表：

![](http://i.imgur.com/7Bu2KjN.png)

    >>> precision = TP/(TP+FP) # 模型识别正样本的正确率
    >>> recall = TP/(TP+FN) # 模型找出正确的正样本的能力

![](http://i.imgur.com/PmLzipZ.png)

我们应该如何对precision进行优化，我们采取了绘制precision-recall曲线的方法：

    >>> from sklearn.metrics import precision_recall_curve
	>>> precision, recall, thresholds = precision_recall_curve(y_test,
	clf.predict(X_test))

![](http://i.imgur.com/AfUNAjq.png)

可以看到，我们的模型对于错误答案的预测能力十分有限。 对于错误答案的预测基本上停留在0.6上下的水平，对于那些正确答案，预测能力则由达到比较高程度的可能。 然而根据我们的需求，正确预测错误答案的能力有多大其实都并非我们所关心的。

- 图中的auc值代表了阴影面积的比例，也是一种量化precision-recall关系的手段。

下面我们用十分简单的手段就可以提取出限定precision后的R值以及对应的thresh:

	>>> thresholds = np.hstack(([0],thresholds[medium]))
	>>> idx80 = precisions>=0.8
	>>> print("P=%.2f R=%.2f thresh=%.2f" % \ (precision[idx80][0],
	recall[idx80][0], threshold[idx80][0]))
	P=0.81 R=0.37 thresh=0.63

这样，我们的模型的precision就能够超过0.8，同时召回率为0.37。 这说明我们的模型差不多只能从三个优秀答案中辨别出一个优秀答案（recall仅有0.37）。 但是，一旦辨别成功，我们对于这个结果还是相当有把握的（precision高达0.8以上）。

为了将得到的thresh应用到我们的模型中，我们可以调出logistic回归对于每个观测的预测概率，对于在thresh以上的部分，我们认定为是优秀答案，其他全是坏答案。

## 分类器的简化 ##

我们可以查看logsitic回归中，各个特征的贡献度，我们直接通过查看对应特征之前的系数来确定：

![](http://i.imgur.com/azr83Xh.png)

数值越大说明特征的贡献度越大。 与现实不太相符的地方在于中部的图片数量特征，图中显示，答案中的图片数量和答案好坏几乎无关，这可能是由于带有图片的答案太少，特征过于稀疏的原因。 对于这样的特征，我们完全可以舍弃。