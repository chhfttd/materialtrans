我们已经对于最小二乘回归(OLS)相当熟悉。我们将于本章中首先了解如何利用numpy和scikit-learn实现最小二乘回归。 但是在实际问题中经典方法总是在运用上困难重重，比如对于特征数量大于观测数量的数据集，OLS就无法得到正确的结果。 所以在过去的10年之内，我们发展了像是Lasso, Ridge, Elastic Net这样的回归方法。

## 利用回归方法预测房价 ##

我们从一个简单的例子开始，预测Boston房价。 我们可以利用公开的数据集，数据集的特征包含了一系列的人口学参数和地理学参数，例如地区的师生比和地区的犯罪率。 我们的目标是预测某个区域内房屋价格的中位值。 我们现在有训练集，且训练集已经标注了房价。

boston数据集是scikit-learn的一个内建数据集。我们可以用一般的方法直接调用：

    from sklearn.datasets import load_boston
	boston = load_boston()

boston对象中对我们最有用的方法无外乎boston.data和boston.target。 我们这里先将boston数据集中的平均房间数量和平均房屋价格放在一起进行一维的回归，这里采用的回归是最为普通的最小二乘技术：

	from matplotlib import pyplot as plt
	plt.scatter(boston.data[:,5], boston.target, color='r')

这里先做出了二者之间的散点图。

	import numpy as np
	x = boston.data[:,5]
	x = np.array([[v] for v in x])

这里，我们取出了作为自变量的平均房间数量。

	y = boston.target
	slope,_,_,_ = np.linalg.lstsq(x,y)

随后，我们取出房屋价格，再利用linalg的lstsq方法实现OLS回归。随后，我们先取出回归得到的直线斜率。之后做出我们的回归直线。

![](http://i.imgur.com/xLM9TBO.png)

效果并不好，但是我们可以估计出二者之间实际上存在一个倍数的关系。 接下来一般的处理方法是在原式加上一个bias(之前没有引入常数项，这里引入了而已)。 

    x = boston.data[:,5]
	x = np.array([[v,1] for v in x]) # we now use [v,1] instead of [v]
	y = boston.target
	(slope,bias),_,_,_ = np.linalg.lstsq(x,y）

![](http://i.imgur.com/PcqioCa.png)

一般我们都希望去量化回归的误差，我们在这里可以这样实现：

	(slope,bias),total_error,_,_ = np.linalg.lstsq(x,y)
	rmse = np.sqrt(total_error[0]/len(x))

lstsq方法返回的是残差平方和，我们在这里把它转化成了标准差。 先前我们进行的无常数项的回归标准差达到了7.6，这里引入了常数项标准差为6.6。 这意味着我们的模型对于价格的估计是非常非常不准确的。

## 多维回归 ##

我们现在要引入更多的因素来预测房价。制作一个多输入一个输出的模型。
    x = boston.data
	x = np.array([np.concatenate((v,[1])) for v in boston.data])

用此x矩阵进行线性回归得到的标准差为4.7，纳入了所有数据之后，模型的精度更高了。

## 回归分析的交叉验证 ##

我们接下来将要使用linearregression类来实现交叉验证：

	from sklearn.linear_model import LinearRegression
	lr = LinearRegression(fit_intercept=True)

这里设置了fit_intercept=True是为了在回归类内引入一个常数项。  之后的工作就是训练模型，求算误差：

    lr.fit(x,y)
	p = map(lr.predict, x)

这里我们用了一个间接的方法来计算模型的回归值。

	e = p-y
	total_error = np.sum(e*e) # sum of squares
	rmse_train = np.sqrt(total_error/len(p))

接下来我们将要使用KFold类来实现一次10-fold交叉验证：

	import numpy as np
	from sklearn.datasets import load_boston
	from sklearn.cross_validation import KFold
	from sklearn.linear_model import LinearRegression
	from sklearn.metrics import mean_squared_error, r2_score
	
	boston=load_boston()
	x=boston.data
	x=np.array([np.concatenate((v,[1])) for v in x])
	y=boston.target
	err = 0
	kf=KFold(len(x), n_folds=10)
	lr=LinearRegression(fit_intercept=True)
	for train,test in kf:
		lr.fit(x[train],y[train])
		p = map(lr.predict, x[test])
		err += mean_squared_error(y[test], p)
	rmse_10cv = np.sqrt(err/len(kf))
	print rmse_10cv

这样，就实现了十折交叉验证，并且计算了标准差为5.9。

## 惩罚回归 ##

线性回归的一个重要的变体的惩罚回归。 在一般的回归中，直线对训练集实现最优拟合，所以可能产生过拟合的情况（将训练集中的噪音采纳为特征）。 常用的惩罚分为l1惩罚和l2惩罚。 l1惩罚下，我们用系数的绝对值之和来惩罚我们的回归；l2惩罚下，我们则使用平方和。

在一般的回归中，我们确定直线的准则如下：

![](http://i.imgur.com/Uog4TOJ.png)

但是在惩罚回归中，我们在目标函数后引入惩罚项：

![](http://i.imgur.com/rvQJPeU.png)

![](http://i.imgur.com/wa2FjEU.png)

这里，可以知道，我们的想法是，不仅要确定系数使得误差最小。同时也要尽可能地使得系数的绝对值和或者平方和最小。

- l1惩罚回归也叫lasso回归；l2惩罚回归也叫岭回归；将l1和l2的惩罚函数组合起来做成新的惩罚函数从而实现的回归称作Elastic网络。

**Lasso回归** 除了能给出比较小的回归系数之外，还可以返回一个带有很多0的回归系数集。这意味着，最终的模型中甚至可以略去一些特征——模型变得更加稀疏了。 这个方法可以将特征选择和回归模型的建立放在同一个步骤之内。

对于惩罚项前的系数：惩罚项越大，我们的模型和一般OLS的差距就越大。

## 在scikit-learn中实现：惩罚回归 ##

在scikit-learn中，我们很容易实现Elastic Net:

	from sklearn.linear_model import ElasticNet
	en = ElasticNet(fit_intercept=True, alpha=0.5)

## P大于N的情形 ##

P在这里代表的是特征数目,N则代表了观测数。 P大于N的情形，也就对应了我们常说的维灾。 这种情况实际上也常有发生，比如利用bag-of-words方法制作得到的数据就常常是P大于N的。 P大于N型问题最大的弊端就是可能产生无穷多组最优参数。 同时会给出0标准差，而0标准差没有任何参考价值。

## 一个基于文本的例子 ##

我们所使用的示例数据是一个适用于svm分类器的数据集。 所以我们用load_svmlight_file方法来读取数据：

	from sklearn.datasets import load_svmlight_file
	data,target = load_svmlight_file('E2006.train')

在这个数据集中，X矩阵是一个稀疏矩阵而y是一个列向量。 利用和上述方法类似的手段可以计算在该数据集上做线性回归的误差，约为0.0024——非常的小；但是进行交叉验证然后求算误差可以得到为0.78左右。 但是由于数据集本身具有0.6左右的误差（？），所以只要我们用线性回归估计平均值，就永远存在这么大的误差。 可见，线性回归中的误差——模型训练误差，是不显著的。 **当进行交叉验证的时候，我们得到的是一般化误差，它可能数值会比较大。** 事实上，这也是P大于N模型的一个问题：OLS的训练误差在这种数据中可能会很小，但是交叉验证误差会很。

## 设置超参数 ##

在前面的章节中，我们将惩罚部分的参数设置成了1。显然，参数过大会导致欠拟合而参数过小模型由又过于接近线性回归。 那么在机器学习工作中如何进行参数的选择呢？ 一般是人为选定几个参数，之后逐个地带入模型作交叉验证。 

- 有一点在这里十分值得我们注意：为了能够正确地评估模型的一般化，我们必须进行两个级别的交叉验证：第一级别的交叉验证评估模型的一般性；第二个来确定最优系数。比如我们进行10折交叉验证，我们第一个层面的验证就应该保留其中的一个部分作为测试集，而在其他9个部分上训练模型。 之后再将每个部分分成10分来选出最佳参数。 所有的训练结束之后，我们才在最初的那个测试集上进行模型的测试。 

好在上述比较复杂的交叉验证在scikit-learn中实现起来比较简单，像LassoCV,RidgeCV和ElasticNetCV都封装了用于确定参数的交叉验证，所以这些方法都不需要设置超参数lambda。 

    from sklearn.linear_model import ElasticNetCV
	met = ElasticNetCV(fit_intercept=True)

随后，我们只需要手动设置检验一般化误差部分的交叉验证就可以了：

	kf = KFold(len(target), n_folds=10)
	for train,test in kf:
		met.fit(data[train],target[train])
		p = map(met.predict, data[test])
		p = np.array(p).ravel()
		e = p-target[test]
		err += np.dot(e,e)
	rmse_10cv = np.sqrt(err/len(target))

## 分级预测和推荐 ##



