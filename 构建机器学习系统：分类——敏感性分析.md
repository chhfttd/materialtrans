对于公司而言，监控公众对于关键事件的反应至关重要。 利用实时分析方法和推特上的用户产生的数据，对推特进行敏感性分类成为了可能。 这有的时候叫做观点挖掘。 

## 宏伟蓝图 ##

因为推特特殊的140个字符的限制，关于推特的敏感性分析十分困难。所以典型的敏感性分析方法在这里无效了。 

我们在这里将要做：

- 引入Naive Bayes
- 解释Part Of Speech如何工作以及如何帮助我们
- 展示更多的scikit-learn的功能

## 获取推特数据 ##

这里利用Nick Sanders基于5000推文制作的语料库。

## 介绍朴素贝叶斯分类器 ##

朴素贝叶斯大概是常用的机器学习算法中最为优雅的。 它被证实对于不相关的特征十分健壮，几乎忽略了这些特征。同时也不需要很多的内存。可朴素二字从何而来呢？

朴素二字说明了分类器的最佳工作环境：所有特征都独立。在真实世界中，这样的数据集几乎不存在。但是在实际应用中，即便是独立的条件不满足，贝叶斯分类器的效果也很好。

## 了解贝叶斯理论 ##

为了便于理解，我们先做出以下约定：

![](http://i.imgur.com/59AHoQf.png)

![](http://i.imgur.com/N80Ugdx.png)



- p(c)是先验概率，是不知道数据的前提下的概率。可以简单地通过从训练集中计算某个类别所占据的比例来获得。
- p(F1,F2)是实证信息，或者叫特征F1和F2的联合概率。可以从训练集中计算同时拥有F1和F2特征的观测的比例得到。
- p(F1,F2|c)：似然性是最难确定的部分。他表达了在我们知道观测属于c类的前提下，我们能够在c类中同时找到F1和F2两个特征的概率。

## 了解朴素 ##

![](http://i.imgur.com/3tJoBrs.png)

我们获得了这样一个公式但是公式中的两个条件概率——似然性都十分难以计算。此时，我们要引入“朴素”，我们在此如果认为F1和F2都是独立的。那么我们可以进行如下的改写

- p(F1,F2|c)=p(F1|c)p(F2|c)

随后我们可以得到这样一个公式：

- p(c|F1,F2)=p(c)p(F1|c)p(F2|c)/p(F1,F2)

## 利用朴素贝叶斯分类器进行分类 ##

![](http://i.imgur.com/UCdroZm.png)

但是朴素贝叶斯分类器并不计算上述公式中的任何一个概率。我们只需要找出能满足下式的C的类别即可：

![](http://i.imgur.com/WsCLMmG.png)

但是，在下面我们先对各个概率进行计算来了解贝叶斯分类器。现在我们假设所有推文只可能出现两个单词‘awesome’和‘crazy’,同时每条推文都被分了类别：

![](http://i.imgur.com/FXF158w.png)

![](http://i.imgur.com/GSXDtDk.png)

上式可以说明，在不知道推文信息的情况下直接猜测推文为positive分类更加可能猜对。 但是现在p(F1|c)，p(F2|c)两个概率还不知道。 下面，我们来计算这些概率，例如下式：

![](http://i.imgur.com/5LrQ1U5.png)

上式计算的是在知道推文的分类是pos时，该推文中含有1个awesome的概率，为0.75。对应地，已知推文属于pos分类时，推文中不含有awesome的概率为0.25。这样我们就能计算出所有条件概率，进而我们可以计算F1和F2的联合概率。

![](http://i.imgur.com/mw0i11A.png)

之后根据贝叶斯公式我们可以计算出下列概率：

![](http://i.imgur.com/FAMnB3p.png)

值得注意的是表中最后一行，我们由于出现了0/0而无法解释，所以我们将在接下来的章节中给出解决方案。


## 将词汇的类别纳入分类器 ##

我们要利用语言学分析方法来分析推文。

## 决定词的类别 ##

这是POS(part of speech)的工作内容——决定次的类别。 POS tagger能解析一整句话，之后将它制作成一个相关的树——每个节点都是一个单词，父类-子类关系决定词汇的归属。根据这个树，我们可以决定词汇类别：比如book是noun还是verb。我们在接下来将要使用nltk进行POS工作。

	>>> import nltk
	>>> nltk.pos_tag(nltk.word_tokenize("This is a good book."))
	[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('book', 'NN'), ('.', '.')]

这里先将一句话进行word级别的分词处理，之后再进行POS tag：

	>>> nltk.pos_tag(nltk.word_tokenize("Could you please book the flight?"))
	[('Could', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('book', 'NN'), ('the', 'DT'), ('flight', 'NN'), ('?', '.')]

每个单词后面的标记代表了该字符的类别，具体类别详见NLTK的对应表格。 这样我们就很容易地过滤出我们想要的词汇类别，NN代表名词，VB代表动词，JJ形容词，RB副词。

## 使用SentiWordNet ##

这里获取了每个英语词汇的正评分和负评分。对于感情色彩比较复杂的词汇，正评分和负评分都不为零。

![](http://i.imgur.com/cCw5NjY.png)

这就是SentiWordNet制作的文件的一览表。从pos一列中我们可以观察到词性信息。随后，利用公式1-pos-neg则可以计算词汇的中性度信息。SynsetTerm列则包括了所有和对应词汇成近义词的词汇。 synsetTerm下的词汇中可能出现同一词汇后面带有不同序号的情况，代表了词汇的多含义情况。 我们在这里仅仅将词汇的pos和neg评分分别取算数平均来代表某个特定词汇的评分，而不去对词汇具体的synset归属加以区分。

## 第一个估计器 ##

我们计划从sklearn.base中选取BaseEstimator类作为父类，继承它并创设子类LinguisticVectorizer

